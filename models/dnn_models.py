import math
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import warnings


def no_grad_trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor
    

class UNet(nn.Module):
    def __init__(self, in_channels=8, out_channels=1, features=[16, 32, 64], in_size=41):
        super().__init__()
        self.coff = nn.Parameter(torch.zeros(1, in_channels, in_size)).to('cuda')
        no_grad_trunc_normal_(self.coff, std=.02)
        self.down_convs = nn.ModuleList()
        self.up_convs = nn.ModuleList()
        self.down_batch = nn.ModuleList()
        self.up_batch = nn.ModuleList()
        self.convs_inup = nn.ModuleList()
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(p=0.7)  # or another dropout rate
        
        # Down-sampling path
        for feature in features:
            conv = nn.Conv1d(in_channels, feature, kernel_size=3, padding=1)
            self.down_convs.append(conv)
            batchnorm = nn.BatchNorm1d(num_features=feature)
            self.down_batch.append(batchnorm)
            in_channels = feature

        # Up-sampling path
        reversed_features = list(reversed(features))
        for idx in range(len(reversed_features) - 1):
            input_channels = reversed_features[idx]
            output_channels = reversed_features[idx] // 2
            if idx < len(reversed_features) - 2:
                upconv = nn.ConvTranspose1d(input_channels, output_channels, kernel_size=3, padding=1, stride=2, output_padding=1)
            else:
                upconv = nn.ConvTranspose1d(input_channels, output_channels, kernel_size=3, padding = 0, stride=2, output_padding=0)
            self.up_convs.append(upconv)
            conv = nn.Conv1d(output_channels * 2, output_channels, kernel_size=3, padding="same")
            self.convs_inup.append(conv)
            batchnorm = nn.BatchNorm1d(num_features=output_channels)
            self.up_batch.append(batchnorm)

        # Final output layer
        self.final_conv = nn.Conv1d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        # Down-sampling path
        x = x + self.coff
        skip_connections = []
        for idx, (conv, batchnorm) in enumerate(zip(self.down_convs, self.down_batch)):
            if idx < len(self.down_convs) - 1:
                x = self.activation(batchnorm(conv(x)))
                if idx < len(self.down_convs) - 1:
                    skip_connections.append(x)
                x = self.dropout(x) 
                x = self.pool(x)
            else:
                x = self.activation(batchnorm(conv(x)))
                x = self.dropout(x) 

        # Up-sampling path
        for idx, (upconv, conv, batchnorm) in enumerate(zip(self.up_convs, self.convs_inup, self.up_batch)):
            x = upconv(x)
            skip_connection = skip_connections[-(idx+1)]
            x = torch.cat((x, skip_connection), dim=1)
            x = self.activation(batchnorm(conv(x)))
            x = self.dropout(x) 

        return self.activation(self.final_conv(x)).squeeze(1) 
    
    
class Mlp(nn.Module):
    def __init__(self, in_dim=8, out_dim=1, hid_dim=64, n_hid_layers=2, norm_layer=nn.BatchNorm1d):
        super().__init__()
        self.activation = nn.ReLU()  # could also be ReLU or LeakyReLU
        assert in_dim > 0 and out_dim > 0 and hid_dim > 0 and n_hid_layers > 0  
        self.dims = [in_dim] + n_hid_layers * [hid_dim] + [out_dim]
        self.linears = nn.ModuleList([
            nn.Linear(self.dims[idx - 1], self.dims[idx]) 
            for idx in range(1, len(self.dims))])
        
        norm_layers = []
        for idx in range(1, len(self.dims)):
            if idx == len(self.dims) - 1:
                norm_layers.append(nn.Identity())
            else:
                norm_layers.append(norm_layer(hid_dim))
        self.norm_layers = nn.ModuleList(norm_layers)   

    def forward(self, x):
        for idx in range(len(self.dims) - 1):
            if idx == len(self.dims) - 2:
                x = x + identidy
            x = self.linears[idx](x)
            x = self.norm_layers[idx](x)
            x = self.activation(x)
            if idx == 0:
                identidy = x.clone()
        return x
    
    
if __name__ == '__main__':
    x = torch.ones(16, 29*5)
    model = Mlp()
    y = model(x)
    print(y.shape)